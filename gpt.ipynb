{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSSy2OOX8MOSAqW2PRw6Y+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erfan7emz/NeuralNet/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "UUfyjWDxhCHu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "words = open('names.txt', 'r').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s, i in stoi.items()}"
      ],
      "metadata": {
        "id": "kB9S9WwUhMQV"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [] #input to the nn\n",
        "ys = [] #correct next char\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    i1 = stoi[ch1]\n",
        "    i2 = stoi[ch2]\n",
        "    xs.append(i1)\n",
        "    ys.append(i2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print('number of examples: ', num)\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)  # initialize 27 neurons' weights. each neuron receives 27 inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVqoKhKsmY3z",
        "outputId": "83053825-82d6-48bc-9836-89b3e1a60377"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples:  228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(100):\n",
        "\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float() # input to the nn\n",
        "  logits =xenc @ W # log-counts\n",
        "  counts = logits.exp() #equivalent to counts\n",
        "  # the next two lines are called softmax ,a function to normalize probabilities\n",
        "  probs = counts / counts.sum(1, keepdims=True)\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # vectorize (and better) approach for calculating loss\n",
        "  print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqryFFp4mw1L",
        "outputId": "81bceaf5-ab86-4b1f-df59-59bf9e7f1595"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4829699993133545\n",
            "2.4829440116882324\n",
            "2.482919216156006\n",
            "2.482893943786621\n",
            "2.4828693866729736\n",
            "2.4828450679779053\n",
            "2.482821226119995\n",
            "2.482797622680664\n",
            "2.482774257659912\n",
            "2.4827513694763184\n",
            "2.4827287197113037\n",
            "2.482706308364868\n",
            "2.482684373855591\n",
            "2.4826626777648926\n",
            "2.4826409816741943\n",
            "2.4826197624206543\n",
            "2.4825992584228516\n",
            "2.4825785160064697\n",
            "2.482558250427246\n",
            "2.4825379848480225\n",
            "2.482517957687378\n",
            "2.4824984073638916\n",
            "2.4824795722961426\n",
            "2.4824600219726562\n",
            "2.4824411869049072\n",
            "2.4824228286743164\n",
            "2.4824044704437256\n",
            "2.4823861122131348\n",
            "2.4823684692382812\n",
            "2.4823505878448486\n",
            "2.482333183288574\n",
            "2.4823157787323\n",
            "2.482299327850342\n",
            "2.4822819232940674\n",
            "2.4822654724121094\n",
            "2.4822492599487305\n",
            "2.4822328090667725\n",
            "2.4822168350219727\n",
            "2.482201099395752\n",
            "2.482185125350952\n",
            "2.4821696281433105\n",
            "2.4821548461914062\n",
            "2.4821395874023438\n",
            "2.4821243286132812\n",
            "2.482109785079956\n",
            "2.482095241546631\n",
            "2.4820809364318848\n",
            "2.4820668697357178\n",
            "2.4820523262023926\n",
            "2.482038736343384\n",
            "2.482024908065796\n",
            "2.4820117950439453\n",
            "2.4819977283477783\n",
            "2.4819846153259277\n",
            "2.481971502304077\n",
            "2.4819588661193848\n",
            "2.481945753097534\n",
            "2.481933116912842\n",
            "2.4819204807281494\n",
            "2.481908082962036\n",
            "2.481895923614502\n",
            "2.4818835258483887\n",
            "2.481872081756592\n",
            "2.4818599224090576\n",
            "2.4818482398986816\n",
            "2.4818365573883057\n",
            "2.481825113296509\n",
            "2.481813669204712\n",
            "2.481802463531494\n",
            "2.4817914962768555\n",
            "2.4817800521850586\n",
            "2.481769561767578\n",
            "2.4817588329315186\n",
            "2.481748342514038\n",
            "2.4817376136779785\n",
            "2.4817276000976562\n",
            "2.481717109680176\n",
            "2.4817070960998535\n",
            "2.481696844100952\n",
            "2.48168683052063\n",
            "2.4816770553588867\n",
            "2.4816672801971436\n",
            "2.4816575050354004\n",
            "2.4816477298736572\n",
            "2.481638193130493\n",
            "2.4816291332244873\n",
            "2.4816195964813232\n",
            "2.4816105365753174\n",
            "2.4816012382507324\n",
            "2.4815924167633057\n",
            "2.481583595275879\n",
            "2.4815752506256104\n",
            "2.4815661907196045\n",
            "2.481557607650757\n",
            "2.48154878616333\n",
            "2.4815404415130615\n",
            "2.481531858444214\n",
            "2.4815237522125244\n",
            "2.481515407562256\n",
            "2.4815073013305664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nlls = torch.zeros(5)\n",
        "# for i in range(5):\n",
        "#   x = xs[i].item()\n",
        "#   y = ys[i].item()\n",
        "#   print('--------')\n",
        "#   print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}, {y})')\n",
        "#   print('input to the neural net: ', x)\n",
        "#   print('output probabilies from the neural net', probs[i])\n",
        "#   print('label (actual next character):', y)\n",
        "#   p = probs[i, y]\n",
        "#   print('probability assigned by the net to the correct character:', p.item())\n",
        "#   logp = torch.log(p)\n",
        "#   print('log likelihood:', logp.item())\n",
        "#   nll = -logp\n",
        "#   print('negative log likelihood:', nll.item()) # higher is worse because it means bigger loss\n",
        "#   nlls[i] = nll\n",
        "\n",
        "# print('=========')\n",
        "# print('average negative log lokelihood, i.e. loss =', nlls.mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AWQgrODqbsK",
        "outputId": "9254d4ec-7048-43e3-c86c-3f7fb63052b9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "bigram example 1: .e (indexes 0, 5)\n",
            "input to the neural net:  0\n",
            "output probabilies from the neural net tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
            "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
            "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
            "label (actual next character): 5\n",
            "probability assigned by the net to the correct character: 0.01228625513613224\n",
            "log likelihood: -4.399273872375488\n",
            "negative log likelihood: 4.399273872375488\n",
            "--------\n",
            "bigram example 2: em (indexes 5, 13)\n",
            "input to the neural net:  5\n",
            "output probabilies from the neural net tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
            "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
            "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the correct character: 0.018050700426101685\n",
            "log likelihood: -4.014570713043213\n",
            "negative log likelihood: 4.014570713043213\n",
            "--------\n",
            "bigram example 3: mm (indexes 13, 13)\n",
            "input to the neural net:  13\n",
            "output probabilies from the neural net tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label (actual next character): 13\n",
            "probability assigned by the net to the correct character: 0.026691533625125885\n",
            "log likelihood: -3.623408794403076\n",
            "negative log likelihood: 3.623408794403076\n",
            "--------\n",
            "bigram example 4: ma (indexes 13, 1)\n",
            "input to the neural net:  13\n",
            "output probabilies from the neural net tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
            "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
            "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
            "label (actual next character): 1\n",
            "probability assigned by the net to the correct character: 0.07367686182260513\n",
            "log likelihood: -2.6080665588378906\n",
            "negative log likelihood: 2.6080665588378906\n",
            "--------\n",
            "bigram example 5: a. (indexes 1, 0)\n",
            "input to the neural net:  1\n",
            "output probabilies from the neural net tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
            "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
            "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
            "label (actual next character): 0\n",
            "probability assigned by the net to the correct character: 0.014977526850998402\n",
            "log likelihood: -4.201204299926758\n",
            "negative log likelihood: 4.201204299926758\n",
            "=========\n",
            "average negative log lokelihood, i.e. loss = 3.7693049907684326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  out = []\n",
        "  ix = 0\n",
        "  while True:\n",
        "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    p = counts / counts.sum(1, keepdims=True) # probability for next character\n",
        "\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix ==0:\n",
        "      break\n",
        "  print(''.join(out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqeJ_HQt3GQU",
        "outputId": "d76a5600-a67f-4a33-f797-66d9071bd916"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "morvann.\n",
            "akela.\n",
            "az.\n",
            "arileri.\n",
            "chaiadayra.\n"
          ]
        }
      ]
    }
  ]
}